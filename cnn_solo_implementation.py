# -*- coding: utf-8 -*-
"""CNN Solo Implementation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s86mPHSbYVbV8g-g7Rfw-RbKkKB96ElO
"""

import urllib
import numpy as np 
import pandas as pd 
import re
DATA_FILE = "file.csv"

response  = urllib.request.urlopen("https://raw.githubusercontent.com/madewithml/basics/master/data/news.csv")
html = response.read()
with open(DATA_FILE, 'wb') as fp:
  fp.write(html)

df = pd.read_csv(DATA_FILE)
df.head(5)

X = df['title'].values
y = df['category'].values
print(X[:5])

"""##Process Data"""

import collections
from sklearn.model_selection import train_test_split
TRAIN_SIZE = 0.70
TEST_SIZE = 0.15
VAL_SIZE = 0.15
SHUFFLE = True

def split_data(X, y, TRAIN_SIZE, TEST_SIZE, VAL_SIZE):
  X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=TEST_SIZE, stratify=y, shuffle=SHUFFLE)
  X_train, X_val, y_train, y_val = train_test_split(
      X_train, y_train, test_size=VAL_SIZE, stratify=y_train, shuffle=SHUFFLE)  
  return X_train, X_test, X_val, y_train, y_test, y_val

#Creating Data Splits 
X_train, X_test, X_val, y_train, y_test, y_val = split_data(X, y, TRAIN_SIZE, TEST_SIZE, VAL_SIZE)

"""##Tokenizer using One-hot encoding"""

# Commented out IPython magic to ensure Python compatibility.
# Use TensorFlow 2.x
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
print("GPU Available: ", tf.config.list_physical_devices('GPU'))

FILTERS = "!\"'#$%&()*+,-./:;<=>?@[\\]^_`{|}~"
LOWER = True
CHAR_LEVEL = True

#Decode 
def decode(indices, tokenizer):
  return " ".join(tokenizer.index_word[index] for index in indicies)

#Decode One_hot
def decode_one_hot(seq, tokenizer):
  indices = np.argmax(seq[one_hot] for one_hot in seq)
  return decode(indices=indices, tokenizer=tokenizer)

X_tokenizer = Tokenizer(filters=FILTERS,
                        char_level=CHAR_LEVEL,
                        oov_token='<UNK>',
                        lower=LOWER)

X_tokenizer.fit_on_texts(X_train)
vocab_size = len(X_tokenizer.word_index) + 1
print(vocab_size)

#To Sequencing
Xs_train = np.array(X_tokenizer.texts_to_sequences(X_train))
Xs_test = np.array(X_tokenizer.texts_to_sequences(X_test))
Xs_val = np.array(X_tokenizer.texts_to_sequences(X_val))

#TO One-Hot
Xh_train = np.array([to_categorical(seq, num_classes=vocab_size) for seq in Xs_train])
Xh_test = np.array([to_categorical(seq, num_classes=vocab_size) for seq in Xs_test])
Xh_val = np.array([to_categorical(seq, num_classes=vocab_size) for seq in Xs_val])

print(Xh_train[0])
print(Xh_train.shape)

"""##Label Encoder"""

from sklearn.preprocessing import LabelEncoder

y_tokenizer = LabelEncoder()

y_tokenizer = y_tokenizer.fit(y_test)
classes = list(y_tokenizer.classes_)
print(classes)

#Convert Tokens to Labels
y_train = y_tokenizer.transform(y_train)
y_test = y_tokenizer.transform(y_test)
y_val = y_tokenizer.transform(y_val)

print(y_train[0])

#Class Weights 
counts = np.bincount(y_train)
class_weights = {i: 1.0/count for i, count in enumerate(counts)}
print(class_weights)

"""##Generators"""

import math
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import Sequence

BATCH_SIZE = 64
FILTER_SIZE = 3

class Generator(Sequence):
  def __init__(self, X, y, batch_size, vocab_size, max_filter_size, shuffle=False):
    self.X = X
    self.y = y
    self.batch_size = batch_size
    self.vocab_size = vocab_size
    self.max_filter_size = max_filter_size
    self.shuffle = shuffle 
    self.on_epoch_end()

  def __len__(self):
    return math.ceil(len(self.X) / self.batch_size)

  def __str__(self):
    return (f"<DataGenerator(" \
                f"batch_size={self.batch_size}, " \
                f"batches={len(self)}, " \
                f"shuffle={self.shuffle})>")
  def __getitem__(self, index):
    batch_indicies = self.epoch_indices[index*self.batch_size:
                                        (index+1)*self.batch_size]
    X, y = self.create_batch(batch_indicies=batch_indicies)
    return X, y

  def on_epoch_end(self):
    self.epoch_indices = np.arange(len(self.X))
    if self.shuffle:
      self.epoch_indices = np.random(self.epoch_indicies) 

  def create_batch(self, batch_indicies):
    X = self.X[batch_indicies]
    y = self.y[batch_indicies]

    #Pad the sequences 
    max_seq_len = max(self.max_filter_size, max([len(x) for x in X]))
    X = pad_sequences(X, padding='post', maxlen=max_seq_len)

training_generator = Generator(
    Xh_train, y_train,batch_size=BATCH_SIZE, vocab_size=vocab_size,
    max_filter_size=FILTER_SIZE, shuffle=False)

test_generator = Generator(
    Xh_test, y_test,batch_size=BATCH_SIZE, vocab_size=vocab_size,
    max_filter_size=FILTER_SIZE, shuffle=False)

val_generator = Generator(
    Xh_val, y_val,batch_size=BATCH_SIZE, vocab_size=vocab_size,
    max_filter_size=FILTER_SIZE, shuffle=False)

"""##Implementing the CNN"""

from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Conv1D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import GlobalMaxPool1D
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Input

max_sequence_size = 8 #words per input
vocab_size = 10 #one-hot sequence dimension
x = Input(shape=(max_sequence_size, vocab_size))
print(x)

NUM_FILTERS = 50
HIDDEN_DIM = 100
DROPOUT_P = 0.1
NUM_CLASSES = len(classes)
print(NUM_CLASSES)

class CNN(Model):
  def __init__(self, filter_size, num_filters,
              hidden_dim, dropout_p, num_classes):
    super(Model, self).__init__(name='cnn')
    
    #Create Convolution Filters
    self.conv = Conv1D(filters=num_filters, kernel_size=filter_size,
                       padding='same')
    self.relu = Activation('relu')
    self.batch_normlization = BatchNormalization()
    self.pool = GlobalMaxPool1D(data_format='channels_last')

    #FC Layers
    self.fc1 = Dense(units=hidden_dim, activation='relu')
    self.dropout = Dropout(rate=dropout_p)
    self.fc2 = Dense(units=num_classes, activation='softmax')

    def call(self, x_in, training=False):
      x_in = tf.cast(x_in, tf.float32)

      z = self.conv(x_in)
      z = self.relu(z)
      z = self.batch_normlization(z)
      z = self.pool(z)

      #FC
      z = self.fc1(z)
      z = self.dropout(z, training=training)
      y_pred = self.fc2(z)
      
      return y_pred

    def summary(self, input_shape):
        x_in = Input(shape=input_shape, name='X')
        summary = Model(inputs=x_in, outputs=self.call(x_in), name=self.name)
        return plot_model(summary, show_shapes=True) # forward pass

model = CNN(filter_size=FILTER_SIZE,
            num_filters=NUM_FILTERS, hidden_dim=HIDDEN_DIM,
            dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)

